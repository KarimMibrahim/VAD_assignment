{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:27:17.205585Z",
     "start_time": "2021-10-29T13:27:17.189508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,accuracy_score, precision_score, recall_score, classification_report, roc_auc_score\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "    ### probably autotagger-like model on the spectrogram? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:27:37.801715Z",
     "start_time": "2021-10-29T13:27:37.780765Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# VAD model\n",
    "class Conv_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, stride=1, pooling=2):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, shape, stride=stride, padding=shape//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mp = nn.MaxPool2d(pooling)\n",
    "    def forward(self, x):\n",
    "        out = self.mp(self.relu(self.bn(self.conv(x))))\n",
    "        #out = self.mp(self.relu(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "class VAD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAD, self).__init__()\n",
    "        self.a_norming = nn.BatchNorm2d(1) \n",
    "        #self.to_db = torchaudio.transforms.AmplitudeToDB() \n",
    "\n",
    "        self.conv1 = Conv_2d(1,32)\n",
    "        self.conv2 = Conv_2d(32,64)\n",
    "        self.conv3 = Conv_2d(64,128)\n",
    "        self.conv4 = Conv_2d(128,256)\n",
    "        \n",
    "        self.a_fc1 =  nn.Linear(10240, 512)\n",
    "        self.a_fc2 = nn.Linear(512, 256)\n",
    "        self.a_fc3 = nn.Linear(256, 128)       \n",
    "\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.logits  = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self,audio_input):\n",
    "        #Audio Branch \n",
    "        #audio_db = self.to_db(audio_input) #[FIX! think need to upgrade torch]\n",
    "        audio_norm = self.a_norming(audio_input) \n",
    "        \n",
    "        x_audio = self.conv1(audio_norm)\n",
    "        x_audio = self.conv2(x_audio)\n",
    "        x_audio = self.conv3(x_audio)\n",
    "        x_audio = self.conv4(x_audio)\n",
    "\n",
    "        x_audio = x_audio.view(x_audio.size(0), -1)\n",
    "        x_audio = F.relu(self.a_fc1(x_audio))\n",
    "        x_audio = F.relu(self.a_fc2(x_audio))\n",
    "        x_audio = F.relu(self.a_fc3(x_audio))\n",
    "        \n",
    "        #Merged Branch\n",
    "        x_audio = self.drop(x_audio)\n",
    "        logits = self.logits(x_audio)\n",
    "        output = torch.sigmoid(logits)\n",
    "        return output, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T12:59:16.019598Z",
     "start_time": "2021-10-29T12:59:16.015794Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# get autotagger\n",
    "def get_VAD(device):\n",
    "    # Define loss and optimizer\n",
    "    vad_model = VAD()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(vad_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    vad_model.to(device)\n",
    "    return vad_model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:26:21.673845Z",
     "start_time": "2021-10-29T13:26:21.648026Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_vad(vad_model, train_loader, optimizer, criterion):\n",
    "    for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
    "        vad_model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        # iterate the training set\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data in tepoch:\n",
    "                #tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "                # I split each input into 3 second segments (those together will make a batch)\n",
    "                mel_in = data[0].to(device)\n",
    "                labels = torch.squeeze(data[1]).to(device)\n",
    "\n",
    "\n",
    "                # Choosing 3 seconds partitioning -> 92 frames\n",
    "                padded_mel = torch.zeros(1,1,128,mel_in.shape[3] + FRAMES_3SEC) #Padding input to have 3 seconds of silence at the end\n",
    "                padded_mel[:,:,:,:mel_in.shape[3]] = mel_in\n",
    "                #num_batches = (padded_mel.shape[3] - FRAMES_3SEC) / BATCH_SIZE # Because we will ignore the first 92 frames\n",
    "\n",
    "                #for batch in np.arange(0,num_batches):\n",
    "                partitioned_mels_3secs = torch.zeros(mel_in.shape[3]-FRAMES_3SEC,1,128,FRAMES_3SEC)\n",
    "                label_centerframe = torch.zeros(mel_in.shape[3]-FRAMES_3SEC, 1)\n",
    "                half_window = int(FRAMES_3SEC/2)\n",
    "                for idx, central_frame in enumerate(np.arange(FRAMES_3SEC,mel_in.shape[3],1)):\n",
    "                    partitioned_mels_3secs[idx,:,:,:] = padded_mel[:,:,:,central_frame-half_window:central_frame+half_window]\n",
    "                    label_centerframe[idx] = labels[central_frame]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs,logits = vad_model(partitioned_mels_3secs) # Should be by frame\n",
    "                loss = criterion(logits, label_centerframe) #Notice, CE in pytorch requires targets as indices\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #_, predicted_idx = torch.max(outputs.data, 1)\n",
    "                #correct += (predicted_idx == labels).sum().item()\n",
    "\n",
    "                # compute epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset pipeline\n",
    "    ### for each sample: \n",
    "        # Load audio/spectrogram\n",
    "        # Load groundtruth on frame-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:26:24.300056Z",
     "start_time": "2021-10-29T13:26:24.275133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining dataset pipeline \n",
    "class VAD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_directory,  device = 'cpu'):\n",
    "        filenames = os.listdir(data_directory)\n",
    "        self.df = pd.DataFrame(filenames)\n",
    "        self.data_directory = data_directory\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_id = self.df.loc[index].values[0]\n",
    "        data = np.load(self.data_directory + str(file_id))\n",
    "        spectrogram = torch.from_numpy(data['mel'])\n",
    "        label = torch.from_numpy(data['labels'])\n",
    "        \n",
    "        # this is to ensure all mels have same shape (padded if missing)\n",
    "        #mel_spec = torch.zeros(1,128,1292) # SET TO MAX LENGTH\n",
    "        #labels_stretched = torch.zeros(1,1292)\n",
    "        if(spectrogram.dim() == 2):\n",
    "            spectrogram = torch.unsqueeze(spectrogram,0)\n",
    "        #mel_spec[:, :, :spectrogram.shape[2]] = spectrogram\n",
    "        #labels_stretched[:, :label.shape[1]] = label\n",
    "\n",
    "        return spectrogram , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:26:24.852781Z",
     "start_time": "2021-10-29T13:26:24.844875Z"
    }
   },
   "outputs": [],
   "source": [
    "# initiating dataloader \n",
    "def initialize_dataloaders(trainDataDir, testDataDir):        \n",
    "    train_instance = VAD_Dataset(trainDataDir)\n",
    "    test_instance = VAD_Dataset(testDataDir)\n",
    "    \n",
    "    # I am setting the batch size to 1, because I will be batching each input file \n",
    "    # by partitioning around moving central frame\n",
    "    train_loader = torch.utils.data.DataLoader(train_instance,batch_size=1,shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_instance,batch_size=1,shuffle=False)\n",
    "    \n",
    "    #validation_instance = UserAwareDataset(\"/home/mounted/implicit_valid_set.csv\",\n",
    "    #                            \"/home/mounted/groundtruths/user_embeds_existing.csv\",\n",
    "    #                           \"/home/mounted/implicit_mels/\")\n",
    "    #valid_loader = torch.utils.data.DataLoader(validation_instance,batch_size=32,shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:26:26.960978Z",
     "start_time": "2021-10-29T13:26:26.955038Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDataDir = \"/home/karim/Desktop/Sonos_Assignment/vad_train_set/data_ready/\"\n",
    "testDataDir = \"/home/karim/Desktop/Sonos_Assignment/vad_test_set/data_ready/\"\n",
    "results_path = \"/home/karim/Desktop/Sonos_Assignment/results/\"\n",
    "model_save_path = \"/home/karim/Desktop/Sonos_Assignment/saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "FRAMES_3SEC = 92\n",
    "#min_val_loss = 10**5 #just initialize with random big number \n",
    "#epochs_no_improve = 0\n",
    "#n_epochs_stop = 10\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))\n",
    "\n",
    "train_loader, test_loader = initialize_dataloaders(trainDataDir, testDataDir)\n",
    "\n",
    "vad_model, optimizer, criterion = get_VAD(device)\n",
    "train_vad(vad_model, train_loader, optimizer, criterion)\n",
    "#results, autotagger_y_pred_prob, test_labels = test_autotagger(autotagger, test_loader, y_test, \n",
    "#                                             label_encoder, labels_list, results_path)\n",
    "\n",
    "\n",
    "model_name = model_save_path + \"whatToCall\"\n",
    "torch.save(autotagger.state_dict(),model_name)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:28:00.267671Z",
     "start_time": "2021-10-29T13:28:00.211700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:28:28.485796Z",
     "start_time": "2021-10-29T13:28:01.999273Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:24:18.831874Z",
     "start_time": "2021-10-29T13:24:18.817656Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:16:25.134803Z",
     "start_time": "2021-10-29T13:16:25.116629Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:03:08.477499Z",
     "start_time": "2021-10-29T13:03:08.456161Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:19:56.385408Z",
     "start_time": "2021-10-29T13:19:56.357330Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:19:58.924336Z",
     "start_time": "2021-10-29T13:19:58.920817Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:20:17.000379Z",
     "start_time": "2021-10-29T13:20:16.996814Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:20:50.072757Z",
     "start_time": "2021-10-29T13:20:50.065634Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:19:19.559965Z",
     "start_time": "2021-10-29T13:19:19.556572Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:19:26.142393Z",
     "start_time": "2021-10-29T13:19:26.134534Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:19:33.904531Z",
     "start_time": "2021-10-29T13:19:33.900850Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:10:19.987925Z",
     "start_time": "2021-10-29T13:10:19.972831Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:13:02.608921Z",
     "start_time": "2021-10-29T13:13:02.604669Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-29T13:13:33.287595Z",
     "start_time": "2021-10-29T13:13:33.284182Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
